\begin{quote}\textit{
Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end,   
we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier.
The approach leverages ``instance-level explanations", measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation.
The workflow is based on three main visual representations and steps:
one based on aggregate statistics to see how data distributes across correct / incorrect decisions;
one based on explanations to understand which features are used to make these decisions;
and one based on raw data, to derive insights on potential root causes for the observed patterns.
The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed.
The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.
}\end{quote}
% The case study from this collaboration demonstrates that the proposed method helps experts derive useful knowledge about the model and the phenomena it describes and also generate useful hypotheses on how a model can be improved.

% Machine learning models are most effective when they can objectively describe and predict real-world phenomena. Effectiveness subsumes two factors: the features of the data used in the predictions are correct and useful, and the decisions made by the model are free of biases. Given large, high-dimensional datasets, modelers and users are faced with the challenge of evaluating the degree of effectiveness of a model and subsequently improve the model so that it better captures their domain knowledge. Current methods lack a systematic strategy for semantic validation of models, where a modeler or a user can identify the reasons behind a modelâ€™s decisions and validate their quality with respect to the feature space. To fill this gap, we propose an interactive workflow based on instance-level visual explanations that serves a two-fold purpose: i) makes the connections between model decisions and features explicit for subsets of instances, and ii) helps modelers and users validate and trust the decisions by exploring the explanations behind the decisions. We validate our approach by creating artificial datasets with known biases and identifying those biases with our method. We also demonstrate the efficacy of our workflow with a long-term case study about improving wait time in the emergency department of hospitals.

% Modern machine learning techniques have made it possible to accurately model almost any phenomenon.
% However, when solving problems using machine learning oftentimes gathering the correct data is the biggest challenge.
% Great care has to be taken to not introduce biases or capture data that does not have the ability to fully solve the problem.
% In order to assist modelers in improving their \textit{data} we propose an interactive workflow alongside a visual user interface utilizing item level explanations of decision made by machine learning models.
% The workflow helps to understand how the model views the data enabling a user to understand the model's and the data's shortcomings and subsequently validate and trust them.
% Furthermore, we validate our approach by creating artificial datasets with known biases and identifying those biases with our method.
% We demonstrate our workflow with a long-term case study about improving wait time in the emergency department of hospitals.

\begin{quote}
\textit{Josua Krause, Aritra Dasgupta, Enrico Bertini}
\end{quote}