\chapter{Thesis Summary}
\label{chap:summary}

In this thesis, we discussed the role of visualization and human interaction \todo{TODO}

However, this approach has some limitations to overcome.
Note, that many of the limitations are not necessarily an argument against the presented techniques but merely a starting point for future research.

\section{Limitations and Assumptions of Black-Box Analysis}
Analyzing machine learning models using black-box techniques can only approximate the true behavior of the model.
Looking at more instances or increasing the sample rate for both partial-dependence plots or instance-level explanations increases the granularity and precision of the analysis but cannot express the underlying relationships of the input in full.
On the other hand, those relationships can be too complex to be understood by a human.
The challenge is to find a middle ground between the fidelity of explanations and their interpretability.

Additionally, some tasks have complex interactions between input features that cannot be described by interpreting individual features.
For example, instance-level explanations provide weights for the features for a given instance.
However, those weights can be completely different for other instances.
Instance-level explanations alone do not provide enough information to reason about why the weights changed in this particular way.
A possible solution could be to expand the concept of explanations to allow for representing non-trivial relationships between features, such as using rules to express partial behaviors of the model or a higher dimensional partial dependence.

Black-box analysis methods provide the convenience of using the same algorithm for multiple models.
However, with regards to data some decisions still need to be made.
For example, for the instance-level explanations in \chapref{chap:explainer} we determined that Martens and Provost's~\cite{Martens:2014:EDD:2600518.2600523} algorithm provided consistently better results than Riberio~\etal's\cite{DBLP:journals/corr/RibeiroSG16} algorithm for our use case of highly sparse binary data.
This is due to the fact that different data types require different approaches for explanations.
A binary feature can have an explanation that describes the presence or absence of the feature whereas for a numeric feature the actual value is important.
Our Model Diagnostic Workflow is agnostic to the used instance-level explanation algorithm but this choice still has to be made.

\section{Generalization to Other Forms of Machine Learning}
\todo{TODO}
* only predictions tasks
* might be possible for LSTM
* gradient to explanation
* other machine learning tasks like reinforcement learning etc.

\section{Implications}
\todo{TODO}

Interestingly, because humans are used to having contextual knowledge about real world problems machine learning is trying to solve, it often seems surprising when and why a model failed, since the correct prediction ``is so obvious".
This also means, that repeatedly seeing that a model performs correctly, especially when providing a reasoning behind the decision in the form of \emph{explanations} might lead to overconfidence of the human in the capabilities of the model.
\todo{explanations considered harmful?}
\todo{TODO}
* automl and the human role in ML
* vis systems from last year
* attention models
* additive models

% \todo{see motivation for building context}
% \todo{discussion? -- also overfitting}


% \todo{end???}
% The model diagnostic workflow proposed in the previous chapter has so far only been tested with binary input features.
% This poses the question on how to expand to general numeric features.
% While it is possible to adapt the used explanation algorithm to accept arbitrary features the resulting explanations are less interpretable.

% \input{figs/new_parallel}
% \input{figs/new_multiclass}

% \section{Extending the Model Diagnostic Workflow}
% In the case of binary features just naming the feature is enough to show its significance and how it is used by the machine learning model.
% For numeric features additional information is needed.
% For example, an explanation would be ``Feature A has high values" which poses further questions of the granularity of the description (``Feature A $>$ 5", ``Feature A $>$ average").
% This introduces ambiguity and vagueness to the explanation.
% A possible solution to this is to describe the explanation \emph{implicitly} by showing the distribution of values within features and how features interconnect.
% The techniques proposed by \cite{seekaview} provide solutions to explore those relations effectively.

% Another problem arising from generalizing the model diagnostic workflow is the observation that explanation algorithms include more features when the features are numerical (\ie, more features are relevant to the prediction).
% The LIME algorithm \cite{DBLP:journals/corr/RibeiroSG16}, which produces feature weights for each instance, overcomes this issue by arbitrarily restricting features to a pre-chosen quantity (\eg, 10 features) by descending importance.
% This restriction is independent of the actual distribution of feature weights.
% It can happen that important features get removed or relatively unimportant features get included in the explanation.

% \input{figs/new_projection}

% With the above mentioned shift away from \emph{explicitly} describing explanations through their features towards \emph{implicitly} describing explanations through their instances, less care has to be taken to keep the number of relevant features in a range that is interpretable.
% This opens up the aggregation of explanations from having explanations match exactly to being able to group together instances that are explained similarly.
% This can be achieved by clustering the feature weights as obtained from the LIME algorithm which solves the problem of arbitrarily cutting off features from explanations as all features are included in the distance metric for clustering.
% The resulting ``decision sets" can then be visualized (see Figures~\ref{figs:new_parallel}, \ref{figs:new_multiclass}, and \ref{figs:new_projection})
% \vspace*{-0.5em}

% \section{Formal Evaluation}
% The model diagnostic workflow proposed in \chapref{chap:explainer} has not been formally evaluated.
% We obtained information about its effectiveness through evidence from its use to improve machine learning models.
% However, this does not guarantee that using explanations for machine learning models actually have a positive impact on the human perception of the quality of a model.
% We are planning to run a study that aims to determine how the model diagnostic workflow changes the perceived trust and confidence in a model.
% Besides subjectively measuring trust and confidence we are presenting decisions made by a model alongside information about their underlying data and asking participants to decide whether the decision is correct.
% In one condition the participant has been exposed to only basic statistical performance measures of the machine learning models.
% In the other condition the participant has the opportunity to use an implementation of our model diagnostic workflow to gain deeper understanding into the strengths and weaknesses of the model.
% We have not started to run the study yet.

% The work shown so far provides an overview of how and where explanations of
% machine learning models can be helpful.
% However, it also shows gaps and challenges that provide much opportunity for further
% research.
% Especially for output only explanation there is still much room for improvements.
% Also, related research for interaction explanations
% (\cite{DBLP:journals/corr/RibeiroSG16} and \cite{Martens:2014:EDD:2600518.2600523})
% show a spectrum of different strategies on how to create those explanations but
% also show that there is still a lack of effective visual analytics tools conveying
% those explanations in a human understandable form.
% Two current projects address those challenges.

% \input{figs/current/paolo_explain}

% \section{Current Projects}
% The first project, in collaboration with Paolo Tamag, addresses the visual analytics
% aspect of providing an effective user interface to explore and understand machine
% generated interaction explanations.
% The interface, shown in \figref{figs:paolo_explain},
% leverages the explanation generation strategy developed by
% \cite{Martens:2014:EDD:2600518.2600523} for explaining predictions
% in text classification tasks.
% It offers explanation exploration to find data points with similar reasons
% for their predicted outcome.
% In the interface the list on the left shows the terms most commonly occurring in explanations.
% After selection explanations containing the selected terms are shown in the middle
% column.
% Information about falsely predicted data points are shown as well.
% Explanations can be further inspected by listing all matching data points (right).
% Terms from the explanation are highlighted.
% Further work needs to be done to utilize the interplay of data driven clusters
% and explanation driven clusters.

% The second project aims to improve output only explanations.
% Here, only prediction scores are used to generate explanations.
% In the current state of the project paths towards the highest scoring item
% are computed on existing data points.
% The resulting tree graph structure can then be explored and used for
% explanation generation.
% This technique helps analysts find and understand clusters of similarly behaving
% data points and regions of great prediction score fluctuation in the item space.
% Initial experiments of visual analytics approaches can be seen in
% \figref{figs:current_adj},
% \figref{figs:current_graph}, and
% \figref{figs:current_burst}.
% Further work needs to be done in terms of summarizing groupings and guidance to
% interesting areas in the item space.

% \input{figs/current/graph}

% \input{figs/current/burst}

% \input{figs/current/adj}

% \chapter{Dissertation Outline and Progress}
% \label{chap:thesis}
% \section{Proposed Outline}
% Below is the proposed outline of the planned dissertation, with references to previous research.

% \begin{enumerate}[label*=\arabic*.]
%     \item Introduction
%     \begin{enumerate}[label*=\arabic*.]
%         \item Machine Learning
%         \begin{enumerate}[label*=\arabic*.]
%             \item Predictive Modeling
%             \item Popular Algorithms
%         \end{enumerate}
%         \item Motivation
%         \item Related Work
%         \item Use Case: Machine Learning for Health Care
%     \end{enumerate}
%     \item Feature Selection for Understanding Models
%     \begin{enumerate}[label*=\arabic*.]
%         \item INFUSE: Interactive Feature Selection for Predictive Modeling of High Dimensional Data (\textbf{publication}) ~\cite{infuse}
%         \item Context
%         \item Lessons Learned
%     \end{enumerate}
%     \item Partial Dependence for Understanding Models
%     \begin{enumerate}[label*=\arabic*.]
%         \item Prospector: Visual Inspection of Black-box Machine Learning Models (\textbf{publication}) ~\cite{prospector16}
%         \item Context
%         \item Lessons Learned
%     \end{enumerate}
%     \item Explanations for Understanding Models
%     \begin{enumerate}[label*=\arabic*.]
%         \item A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations (\textbf{publication}) ~\cite{explainer}
%         \item Context
%         \item Lessons Learned
%     \end{enumerate}
%     \item Generalization and Limitations of Explanations
%     \begin{enumerate}[label*=\arabic*.]
%         \item Generalization of Explanations to Non-Binary Data (\textbf{TBD})
%         \item Study on Impacts of Explanations for Model Confidence and Trust (\textbf{TBD})
%     \end{enumerate}
%     \item Discussion
%     \begin{enumerate}[label*=\arabic*.]
%         \item Limitations and Assumptions of Black-Box Analysis
%         \item Generalization to Other Forms of Machine Learning
%         \item Implications
%     \end{enumerate}
%     \item Conclusion and Future Work
% \end{enumerate}
% \newpage
