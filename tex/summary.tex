\chapter{Thesis Summary}
\label{chap:summary}

In this thesis, we discussed the role of visual analytics to explain black-box machine learning.
We saw that global approaches, like feature selection methods, are not informative enough to help understand model decisions.
Different strategies preferred different, equally reasonable, feature sets without having a significant impact on predictive performance.
This showed, that inspecting and comparing alternate settings may let machine learning experts develop insights that overwrite their initial intuitions.

Next, we saw that partial dependence with a derived feature importance score allowed to effectively detect model errors.
Detectable errors included: over-fitting, under-fitting, biases caused by imputation, and leaking labels caused by incorrect cause-effect relationships.
Furthermore, localized inspections helped to understand the how and why of specific instance predictions by finding locally impactful features.

Then, we proposed and discussed the Model Diagnostic workflow, which is based on aggregating local instance-level explanations in order to gain insights about a model with the intent of improving it.
We showed that the Model Diagnostic workflow enables a scalable understanding of local decision making of a predictive model and that aggregating instance-level explanations allows for semantic validation of the input data.
As a model can only perform as well as its input data, gaining insights about the limitations of a model in turn help with feature engineering on said data.

Finally, we showed how histograms can be effectively used to analyze aggregated instance-level explanations.
With the help of comparing meaningful subsets, aggregated instance-level explanations significantly outperformed inspecting individual explanations or aggregation without explanations in detecting biases in the input data while being more scalable than a tabular representation.
Furthermore, we showed that inspecting individual instance-level explanations can be misleading and hurts detecting biases in the input data.

All in all, we demonstrated a workflow for effectively utilizing black-box instance-level explanations in order to improve model correctness via semantic validation.
However, this approach has some limitations to overcome which we will explore next.
Note, that those limitations are not an argument against the presented techniques but merely a starting point for future research.

\section{Limitations and Assumptions of Black-Box Analysis}
Analyzing machine learning models using black-box techniques can only approximate the true behavior of the model.
Looking at more instances or increasing the sample rate for both partial-dependence plots or instance-level explanations increases the granularity and precision of the analysis but cannot express the underlying relationships of the input in full.
On the other hand, those relationships can be too complex to be understood by a human.
The challenge is to find a middle ground between the fidelity of explanations and their interpretability.

Additionally, some tasks have complex interactions between input features that cannot be described by interpreting individual features.
For example, instance-level explanations provide weights for the features for a given instance.
However, those weights can be completely different for other instances.
Instance-level explanations alone do not provide enough information to reason about why the weights changed in this particular way.
A possible solution could be to expand the concept of explanations to allow for representing non-trivial relationships between features, such as using rules to express partial behaviors of the model or a higher dimensional partial dependence.

Black-box analysis methods provide the convenience of using the same algorithm for multiple models.
However, with regards to data some decisions still need to be made.
For example, for the instance-level explanations in \chapref{chap:explainer} we determined that Martens and Provost's~\cite{Martens:2014:EDD:2600518.2600523} algorithm provided consistently better results than Riberio~\etal's\cite{DBLP:journals/corr/RibeiroSG16} algorithm for our use case of highly sparse binary data.
This is due to the fact that different data types require different approaches for explanations.
A binary feature can have an explanation that describes the presence or absence of the feature whereas for a numeric feature the actual value is important.
Our Model Diagnostic workflow is agnostic to the used instance-level explanation algorithm but this choice still has to be made.

\section{Generalization to Other Forms of Machine Learning}
In this thesis, we only focused on predictive modeling tasks with structured inputs.
Even though this is a large and popular subset, it does not cover the entirety of machine learning or even predictive modeling.
Presented techniques are not immediately transferable to tasks such as online learning or streaming input data, which requires recurrent models, such as LSTM (Long Short Term Memory) models~\cite{Hochreiter:1997:LSM:1246443.1246450}.
Those areas pose further challenges and offer a variety of research opportunities.

On a different note, the recent popularity of neural networks offers an additional opportunity in terms applicability of the work in this thesis.
Neural networks are differentiable models, which makes it possible to compute gradients towards desired outcomes for given inputs.
As instance-level explanations aim to approximate this gradient to some degree (LIME~\cite{DBLP:journals/corr/RibeiroSG16} is essentially a Monte-Carlo approximation of the above mentioned gradient) gradients might be used as drop-in replacements for instance-level explanations in the techniques proposed in this thesis.
However, whether gradients are a computationally faster alternative to instance-level explanations and whether they can achieve similar results poses an interesting open research question.

\section{Implications}
Meta-learning, \ie, using machine learning to learn model architectures for solving the actual task, is recently growing in popularity.
With systems, such as auto-sklearn~\cite{NIPS2015_5872} or AlphaGo~\cite{silver2016mastering}, the role of humans in machine learning shifts away from being the architect of models to being a domain expert of the modeled problem.
Humans have vast contextual knowledge that is hard to communicate directly to machine learning models.
As such, understanding the behavior of black-box machine learning models and semantic validation of their performance, as presented in this thesis, is becoming more and more relevant.

Interestingly, because humans are used to having contextual knowledge about real world problems machine learning is trying to solve, it often seems surprising when and why a model failed, since the correct prediction ``is so obvious".
% This also means, that repeatedly seeing that a model performs correctly, especially when providing a reasoning behind the decision in the form of \emph{explanations} might lead to overconfidence of the human in the capabilities of the model as we have seen in \chapref{chap:aggexplain}.
Thus, it is crucial for machine learning models to correctly communicate their behavior to humans in order to prevent such fallacies to go unnoticed.
This thesis provided an important step towards achieving this goal.

% \todo{see motivation for building context}
% \todo{discussion? -- also overfitting}
% \todo{end???}
% The model diagnostic workflow proposed in the previous chapter has so far only been tested with binary input features.
% This poses the question on how to expand to general numeric features.
% While it is possible to adapt the used explanation algorithm to accept arbitrary features the resulting explanations are less interpretable.

% \input{figs/new_parallel}
% \input{figs/new_multiclass}

% \section{Extending the Model Diagnostic Workflow}
% In the case of binary features just naming the feature is enough to show its significance and how it is used by the machine learning model.
% For numeric features additional information is needed.
% For example, an explanation would be ``Feature A has high values" which poses further questions of the granularity of the description (``Feature A $>$ 5", ``Feature A $>$ average").
% This introduces ambiguity and vagueness to the explanation.
% A possible solution to this is to describe the explanation \emph{implicitly} by showing the distribution of values within features and how features interconnect.
% The techniques proposed by \cite{seekaview} provide solutions to explore those relations effectively.

% Another problem arising from generalizing the model diagnostic workflow is the observation that explanation algorithms include more features when the features are numerical (\ie, more features are relevant to the prediction).
% The LIME algorithm \cite{DBLP:journals/corr/RibeiroSG16}, which produces feature weights for each instance, overcomes this issue by arbitrarily restricting features to a pre-chosen quantity (\eg, 10 features) by descending importance.
% This restriction is independent of the actual distribution of feature weights.
% It can happen that important features get removed or relatively unimportant features get included in the explanation.

% \input{figs/new_projection}

% With the above mentioned shift away from \emph{explicitly} describing explanations through their features towards \emph{implicitly} describing explanations through their instances, less care has to be taken to keep the number of relevant features in a range that is interpretable.
% This opens up the aggregation of explanations from having explanations match exactly to being able to group together instances that are explained similarly.
% This can be achieved by clustering the feature weights as obtained from the LIME algorithm which solves the problem of arbitrarily cutting off features from explanations as all features are included in the distance metric for clustering.
% The resulting ``decision sets" can then be visualized (see Figures~\ref{figs:new_parallel}, \ref{figs:new_multiclass}, and \ref{figs:new_projection})
% \vspace*{-0.5em}

% \section{Formal Evaluation}
% The model diagnostic workflow proposed in \chapref{chap:explainer} has not been formally evaluated.
% We obtained information about its effectiveness through evidence from its use to improve machine learning models.
% However, this does not guarantee that using explanations for machine learning models actually have a positive impact on the human perception of the quality of a model.
% We are planning to run a study that aims to determine how the model diagnostic workflow changes the perceived trust and confidence in a model.
% Besides subjectively measuring trust and confidence we are presenting decisions made by a model alongside information about their underlying data and asking participants to decide whether the decision is correct.
% In one condition the participant has been exposed to only basic statistical performance measures of the machine learning models.
% In the other condition the participant has the opportunity to use an implementation of our model diagnostic workflow to gain deeper understanding into the strengths and weaknesses of the model.
% We have not started to run the study yet.

% The work shown so far provides an overview of how and where explanations of
% machine learning models can be helpful.
% However, it also shows gaps and challenges that provide much opportunity for further
% research.
% Especially for output only explanation there is still much room for improvements.
% Also, related research for interaction explanations
% (\cite{DBLP:journals/corr/RibeiroSG16} and \cite{Martens:2014:EDD:2600518.2600523})
% show a spectrum of different strategies on how to create those explanations but
% also show that there is still a lack of effective visual analytics tools conveying
% those explanations in a human understandable form.
% Two current projects address those challenges.

% \input{figs/current/paolo_explain}

% \section{Current Projects}
% The first project, in collaboration with Paolo Tamag, addresses the visual analytics
% aspect of providing an effective user interface to explore and understand machine
% generated interaction explanations.
% The interface, shown in \figref{figs:paolo_explain},
% leverages the explanation generation strategy developed by
% \cite{Martens:2014:EDD:2600518.2600523} for explaining predictions
% in text classification tasks.
% It offers explanation exploration to find data points with similar reasons
% for their predicted outcome.
% In the interface the list on the left shows the terms most commonly occurring in explanations.
% After selection explanations containing the selected terms are shown in the middle
% column.
% Information about falsely predicted data points are shown as well.
% Explanations can be further inspected by listing all matching data points (right).
% Terms from the explanation are highlighted.
% Further work needs to be done to utilize the interplay of data driven clusters
% and explanation driven clusters.

% The second project aims to improve output only explanations.
% Here, only prediction scores are used to generate explanations.
% In the current state of the project paths towards the highest scoring item
% are computed on existing data points.
% The resulting tree graph structure can then be explored and used for
% explanation generation.
% This technique helps analysts find and understand clusters of similarly behaving
% data points and regions of great prediction score fluctuation in the item space.
% Initial experiments of visual analytics approaches can be seen in
% \figref{figs:current_adj},
% \figref{figs:current_graph}, and
% \figref{figs:current_burst}.
% Further work needs to be done in terms of summarizing groupings and guidance to
% interesting areas in the item space.

% \input{figs/current/graph}

% \input{figs/current/burst}

% \input{figs/current/adj}

% \chapter{Dissertation Outline and Progress}
% \label{chap:thesis}
% \section{Proposed Outline}
% Below is the proposed outline of the planned dissertation, with references to previous research.

% \begin{enumerate}[label*=\arabic*.]
%     \item Introduction
%     \begin{enumerate}[label*=\arabic*.]
%         \item Machine Learning
%         \begin{enumerate}[label*=\arabic*.]
%             \item Predictive Modeling
%             \item Popular Algorithms
%         \end{enumerate}
%         \item Motivation
%         \item Related Work
%         \item Use Case: Machine Learning for Health Care
%     \end{enumerate}
%     \item Feature Selection for Understanding Models
%     \begin{enumerate}[label*=\arabic*.]
%         \item INFUSE: Interactive Feature Selection for Predictive Modeling of High Dimensional Data (\textbf{publication}) ~\cite{infuse}
%         \item Context
%         \item Lessons Learned
%     \end{enumerate}
%     \item Partial Dependence for Understanding Models
%     \begin{enumerate}[label*=\arabic*.]
%         \item Prospector: Visual Inspection of Black-box Machine Learning Models (\textbf{publication}) ~\cite{prospector16}
%         \item Context
%         \item Lessons Learned
%     \end{enumerate}
%     \item Explanations for Understanding Models
%     \begin{enumerate}[label*=\arabic*.]
%         \item A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations (\textbf{publication}) ~\cite{explainer}
%         \item Context
%         \item Lessons Learned
%     \end{enumerate}
%     \item Generalization and Limitations of Explanations
%     \begin{enumerate}[label*=\arabic*.]
%         \item Generalization of Explanations to Non-Binary Data (\textbf{TBD})
%         \item Study on Impacts of Explanations for Model Confidence and Trust (\textbf{TBD})
%     \end{enumerate}
%     \item Discussion
%     \begin{enumerate}[label*=\arabic*.]
%         \item Limitations and Assumptions of Black-Box Analysis
%         \item Generalization to Other Forms of Machine Learning
%         \item Implications
%     \end{enumerate}
%     \item Conclusion and Future Work
% \end{enumerate}
% \newpage
