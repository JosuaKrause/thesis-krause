\section{Machine Learning and Predictive Modeling}
Machine learning enables computers to perform tasks without manually codifying all instructions.
This automated programming makes machine learning a popular and widely used approach for many applications as it provides a time- and cost-effective alternative to manually created software solutions.
Machine learning relies on algorithms that learn and generalize from examples in order to perform accurately on new or unseen data.

Predictive modeling is a subcategory of machine learning that aims to predict certain properties from given input data.
The input data typically consists of \emph{instances} with a given set of \emph{features}.
The values of those features can be binary, categorical, or numerical.
We can think of the data as a table where features are columns and rows are instances.
Thus, this kind of data is typically referred to as \emph{tabular} or \emph{structured} data.
In the case of \emph{unstructured} data, such as plain text, the data needs to be converted into structured data first, before it can be used as input of predictive models.
The predicted property, \emph{outcome} or \emph{label}, can be numerical, in which case the task is called \emph{regression}, or categorical, in which case the task is called \emph{classification}.
The different values the label can assume are called \emph{classes}.
The task is call \emph{binary classification} if the categorical label has exactly two classes.

The predictive model learns by \emph{training} with a set of example instances (\emph{training set}) and their associated labels.
In order to verify that the model did indeed correctly learn from the given examples, a second, previously unseen, set of example instances (training or \emph{validation set}) with known labels is typically used.
The model is used to predict the labels of this set which are then compared to the actual labels of those instances.
Various statistical measures can be computed on this relationship between the predicted and actual labels.

For binary classification tasks (for simplicity \emph{positive} and \emph{negative} are used below to describe the outcomes) those statistical measures include:

\begin{description}[noitemsep]
    \item[True Positive / Negative]
    The number of correctly predicted positive ($TP$) or negative instances ($TN$).
    \item[False Positive / Negative]
    The number of incorrectly predicted positive ($FP$) or negative instances ($FN$).
    \item[Accuracy]
    $(TP + TN) / (TP + TN + FP + FN)$ The proportion of correctly predicted instances.
    \item[Precision]
    $TP / (TP + FP)$ The ratio of correctly predicted positive instances to instances which were predicted to be positive.
    \item[True Positive Rate / Recall]
    $TP / (TP + FN)$ The proportion of correctly predicted positive instances.
    \item[False Positive Rate]
    $FP / (FP + TN)$ The proportion of incorrectly predicted positive instances.
\end{description}

Additionally, a further model quality measurement can be derived from the confidence of a model in its prediction.
Typically, classification models allow to output probabilities for each class instead of the predicted class directly.
This allows for inspecting the \emph{confidence} of the model.
In the case of binary classifiers a \emph{threshold} can be used to adjust predictions to favor confidence in a certain class higher than the other.
This is achieved by outputting the positive class as prediction if its probability is higher than the threshold and the negative class otherwise.

\input{figs/roc}

The \emph{receiver operating characteristic} (ROC) curve for binary classifiers plots the true positive rate against the false positive rate for all thresholds between $0$ and $1$.
The plot indicates the correctness of the model \wrt its confidence.
The ROC curve for a perfect model ($100\%$ accuracy) would follow the left vertical axis and the top horizontal axis.
The area under the ROC curve (\emph{AUC}) is a commonly used measure for model quality. \todo{cite paper???}

\subsection{Popular Algorithms}
There are too many algorithms for predictive models to list all of them here.
However, following we will discuss the most commonly used algorithms.

\subsubsection{Logistic Regression}
\todo{log regr}
- logistic regression

\subsubsection{Additive Models}
\todo{additive models}

\subsubsection{Decision Trees}
\todo{decision trees}

\subsubsection{Ensemble Methods}
\todo{random forest / ensemble methods}

\subsubsection{Neural Networks}
\todo{neural network / multi-layer perceptron}

\subsection{Failure Modes}
\todo{failures}
- overfitting
- underfitting
- data
- model
- unexpected and different from how humans would fail

\section{Visual Analytics}
Gaining meaningful insights about large datasets is hard.
While computing various statistics about the data is often helpful it might oversimplify or in some cases even mislead information about the data.
One example dataset often used to demonstrate this is Ancombe's quartett \todo{cite ancombe}.
The quartett consists of four different datasets that all have the same mean, standard deviation, \todo{what else?}.
However, plotting the values of the datasets reveals that each one of those datasets obeys a different, unique characteriztic \todo{image}.

Visual analytics is the area of studying interactive graphical respresentations for analyzing complex data such as Ancombe's quartett mentioned above.
This is often done in addition to and with the help of computational and statistical procedures.
Various studies (\todo{cite}
% http://info.slis.indiana.edu/~katy/S637-S11/cleveland84.pdf
% preattentive, gestalt
) have been performed to establish visual principles to effectively take advantage of the high processing power of the human visual cortex \todo{?}.
Additionally, interaction with the graphical representations allows to hide some of the information and reveal it through interaction to not overwhelm the analyst.
For example, ``overview first; details on demand" \todo{shneiderman} is a popular mantra taking advantage of interaction.

In the context of machine learning, visual analytics, at first, seems to be a roadblock.
Integrating a human in a machine learning workflow inevitable leads to said human being the bottleneck.
Humans are magnitudes slower than computers which have to idle while waiting for human input.
Thus, human input should only be required for tasks that are impossible without.

\todo{maybe mention hyperparameter / architecture here}
For example, with the goal of improving the accuracy of a model a human should not have the task of, \eg, manually picking the order of the features to be included in a decision tree.
A good intuition might outperform the computer momentarily but having an objective goal makes it possible to eventually find a suitable algorithm that is en par or even outperforms the human.
Additionally, manual decisions are not generalizable to other tasks and not scalable to larger datasets which in turn is the reason to use machine learning to begin with.

However, \todo{}

- accuracy vs. semantic accuracy
- debugging models / data
- domain expert
- understanding /trusting models