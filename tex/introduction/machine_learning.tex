\section{Machine Learning and Predictive Modeling}
Machine learning enables computers to perform tasks without manually codifying all instructions.
This automated programming makes machine learning a popular and widely used approach for many applications as it provides a time- and cost-effective alternative to manually created software solutions.
Machine learning relies on algorithms that learn and generalize from examples in order to perform accurately on new or unseen data.

Predictive modeling is a subcategory of machine learning that aims to predict certain properties from given input data.
The input data typically consists of \emph{instances} with a given set of \emph{features}.
The values of those features can be binary, categorical, or numerical.
We can think of the data as a table where features are columns and rows are instances.
Thus, this kind of data is typically referred to as \emph{tabular} or \emph{structured} data.
In the case of \emph{unstructured} data, such as plain text, the data needs to be converted into structured data first, before it can be used as input of predictive models.
The predicted property, \emph{outcome} or \emph{label}, can be numerical, in which case the task is called \emph{regression}, or categorical, in which case the task is called \emph{classification}.
The different values the label can assume are called \emph{classes}.
The task is call \emph{binary classification} if the categorical label has exactly two classes.

The predictive model learns by \emph{training} with a set of example instances (\emph{training set}) and their associated labels.
In order to verify that the model did indeed correctly learn from the given examples, a second, previously unseen, set of example instances (training or \emph{validation set}) with known labels is typically used.
The model is used to predict the labels of this set which are then compared to the actual labels of those instances.
Various statistical measures can be computed on this relationship between the predicted and actual labels.

For binary classification tasks (for simplicity \emph{positive} and \emph{negative} are used below to describe the outcomes) those statistical measures include:

\begin{description}[noitemsep]
    \item[True Positive / Negative]
    The number of correctly predicted positive ($TP$) or negative instances ($TN$).
    \item[False Positive / Negative]
    The number of incorrectly predicted positive ($FP$) or negative instances ($FN$).
    \item[Accuracy]
    $(TP + TN) / (TP + TN + FP + FN)$ The proportion of correctly predicted instances.
    \item[Precision]
    $TP / (TP + FP)$ The ratio of correctly predicted positive instances to instances which were predicted to be positive.
    \item[True Positive Rate / Recall]
    $TP / (TP + FN)$ The proportion of correctly predicted positive instances.
    \item[False Positive Rate]
    $FP / (FP + TN)$ The proportion of incorrectly predicted positive instances.
\end{description}

Additionally, a further model quality measurement can be derived from the confidence of a model in its prediction.
Typically, classification models allow to output probabilities for each class instead of the predicted class directly.
This allows for inspecting the \emph{confidence} of the model.
In the case of binary classifiers a \emph{threshold} can be used to adjust predictions to favor confidence in a certain class higher than the other.
This is achieved by outputting the positive class as prediction if its probability is higher than the threshold and the negative class otherwise.

\input{figs/roc}

The \emph{receiver operating characteristic} (ROC) curve for binary classifiers plots the true positive rate against the false positive rate for all thresholds between $0$ and $1$.
The plot indicates the correctness of the model \wrt its confidence.
The ROC curve for a perfect model ($100\%$ accuracy) would follow the left vertical axis and the top horizontal axis.
The area under the ROC curve (\emph{AUC}) is a commonly used measure for model quality. \todo{cite paper???}

\subsection{Popular Algorithms}
There are too many algorithms for predictive models to list all of them here.
However, following we will discuss the most commonly used algorithms.

\subsubsection{Logistic Regression}
Logistic Regression is one of the most popular algorithms in machine learning.
This is due to its simplicity and its interpretability of the results.
Suppose $x_i$ is the value of the feature $i$ of our data, a binary classification using Logistic Regression is equivalent to:
\[
l(x) = b + \sum_{i \in \mathcal{C}} w_i x_i
\]
where $\mathcal{C}$ is the set of features, $b$ and $w_i$ are the learned weights of the model, and the outcome is determined by whether the weighted sum $l(x)$ is larger or smaller than zero.
In order to work with probabilities, have greater precision around the boundary, and to penalize points close to the boundary, a logistic function is used:
\[
L(x) = \frac{1}{1 + e^{-l(x)}}
\]
which gives the likelihood of one of the classes.
In order to train the model, \ie, finding the best values of $b$ and $w_f$, the expression $(L(x) - y)^2$, where $y$ represents the ground truth 0 or 1 of the training instances, needs to be minimized.
This can be done with techniques such as gradient descent.

One of the main advantages of logistic regression is its interpretability.
The weights $w_i$ can be directly interpreted as how influential a particular feature is.
That is, if the magnitude of a weight is large, a small change in the value $x_i$ of the feature has a big impact on the overall sum.
Additionally, the sign of a weight indicates whether a particular feature is positively or negatively correlated with the outcome.
For example, a simple (not necessarily correct) predictor for Type 2 Diabetes\footnote{A disease, affecting the body's ability to regulate blood sugar, which is mostly driven by lifestyle choices and associated with obesity and hypertension. The most common age of onset is between ${\sim}45$ and ${\sim}65$.} could look like:
\[
l_\text{diabetes}(x) = -1.9 + 0.01 x_\text{age} + 0.02 x_\text{mass} + (-0.3) x_\text{height}
\]
with $x_\text{age}$ in years, $x_\text{mass}$ in \si{kg}, and $x_\text{height}$ in \si{m}.
From here, we can see that the model assumes that a high mass leads to higher Diabetes risk, old age also leads to a higher risk but less so than high mass, and large body height leads to a smaller risk.
This gives a very intuitive understanding of how the model is reaching its conclusion.

However, this example also illustrates some limitations of logistic regression models.
BMI (Body Mass Index; $x_\text{mass} / x_\text{height}^2$) is a better indicator than mass or height alone, but cannot be expressed or learned by a logistic regression model\footnote{Note, that we could provide BMI directly but only because domain experts had inferred the importance of this relationship in the past. Alternatively, we could also provide features in logarithmic space enabling logistic regression models to learn this relationship, since logarithmic BMI is $\log{(x_\text{mass})} - 2.0 \log{(x_\text{height})}$.}.
Additionally, since the influence of the features is always the same, one feature can force a prediction if its values are large enough.
For example, the model will predict a high Diabetes risk with high age independent of the mass or height.\todo{cite paper}

\subsubsection{Generalized Additive Models}
A Generalized Additive Model (GAM) extends the idea of logistic regression \todo{cite paper; show influence function} to functions instead of weights:
\[
g(x) = b + \sum_{i \in \mathcal{C}} f_i(x_i)
\]
where $\mathcal{C}$ is the set of features, $b$ is the learned bias of the model, $f_i$ are learned influence functions, and the outcome is determined by whether the sum $g(x)$ is larger or smaller than zero.
Having influence functions overcomes the restricted expressiveness of logistic regression models while maintaining its interpretability.
The impact of individual features can be seen in the plots of the influence functions.

In the Diabetes example from above, a GAM could solve the issue of always connecting high age to high Diabetes risk by decreasing the influence of the feature age for higher values than ${\sim}65$.
However, since GAMs only consider one feature at a time, the BMI ($x_\text{mass} / x_\text{height}^2$) can still not be expressed or learned by the model.

\subsubsection{Decision Trees}
\todo{cite paper -- show image}
Instead of computing predictions with a mathematical formula, Decision Trees determine predictions by following a sequence of tests on the data.
Decision Trees are trees where each node represents a test $x_i > t_i$, with $x_i$ as the value of the feature $i$ and $t_i$ as learned threshold.
The prediction algorithm starts at the root of the tree.
Depending on the outcome of the test in the root node the algorithm continues with the corresponding sub-tree recursively, until a leaf node is reached.
The leaf nodes contain the probabilities for the outcomes of the prediction task and are returned as result by the algorithm.

Decision Trees are usually considered very interpretable, as their behaviour is intuitively understandable.
However, this only holds true for trees with a relatively low number of nodes.
Trees with hundreds or thousands of nodes are hard to interpret as their complexity makes it hard to gain a mental model.

Decision Trees excel in cases where certain ranges of features have a clear impact on the outcome.
For example, in our running Diabetes use case, the risk of becoming diabetic is significantly higher for ages between ${\sim}45$ and ${\sim}65$.
A Decision Tree can then differentiate by age and infer the prediction differently for values inside or outside of that range.

However, Decision Trees cannot model relationships between features very well and have to approximate them.
For example, increasing the height of a person also increases that person's weight.
This means that whether a person is overweight, not only depends on the weight of the person, but also how tall that person is.
A person weighing $80\si{kg}$ can be considered overweight with a height of $1.7\si{m}$ but would not be with a height of $1.8\si{m}$.
Since a Decision Tree cannot model this relationship ($x_\text{mass} / x_\text{height}^2 \geq 25$) directly it has to approximate it by breaking the values of $x_\text{mass}$ and $x_\text{height}$ into intervals and checking them separately.
\todo{over-fitting}

\subsubsection{Ensemble Methods}
A powerful method of utilizing several weak models are Ensemble Models.
Instead of taking the prediction of one model, Ensembles combine the prediction of many models in order to get a definite result.
This is typically done by majority voting, \ie, the most common prediction among the individual models is used as final prediction.
\todo{cite papers}

The main advantage of Ensemble Models is that it can improve the performance of otherwise worse performing models.
Given a set of independent / uncorrelated models the error reduction of the Ensemble behaves linearly with number of individual models:
\todo{formula and citation -- chapter regularization}

One popular Ensemble Model is the Random Forest, which combines several Decision Trees trained on random sub-samples of the training data.
\todo{avoid over-fitting; better than Decision Tree}
\todo{link back to running example}

\subsubsection{Neural Networks}
\todo{neural network / multi-layer perceptron}

\subsection{Failure Modes}
Machine learning models not always perform perfectly.
Most \todo{}
\todo{failures}
- overfitting
- underfitting
- data
- model
- unexpected and different from how humans would fail

\section{Visual Analytics}
Gaining meaningful insights about large datasets is hard.
While computing various statistics about the data is often helpful it might oversimplify or in some cases even mislead information about the data.
One example dataset often used to demonstrate this is Ancombe's quartett \todo{cite ancombe}.
The quartett consists of four different datasets that all have the same mean, standard deviation, \todo{what else?}.
However, plotting the values of the datasets reveals that each one of those datasets obeys a different, unique characteriztic \todo{image}.

Visual analytics is the area of studying interactive graphical respresentations for analyzing complex data such as Ancombe's quartett mentioned above.
This is often done in addition to and with the help of computational and statistical procedures.
Various studies (\todo{cite}
% http://info.slis.indiana.edu/~katy/S637-S11/cleveland84.pdf
% preattentive, gestalt
) have been performed to establish visual principles to effectively take advantage of the high processing power of the human visual cortex \todo{?}.
Additionally, interaction with the graphical representations allows to hide some of the information and reveal it through interaction to not overwhelm the analyst.
For example, ``overview first; details on demand" \todo{shneiderman} is a popular mantra taking advantage of interaction.

In the context of machine learning, visual analytics, at first, seems to be a roadblock.
Integrating a human in a machine learning workflow inevitable leads to said human being the bottleneck.
Humans are magnitudes slower than computers which have to idle while waiting for human input.
Thus, human input should only be required for tasks that are impossible without.

\todo{maybe mention hyperparameter / architecture here}
For example, with the goal of improving the accuracy of a model a human should not have the task of, \eg, manually picking the order of the features to be included in a decision tree.
A good intuition might outperform the computer momentarily but having an objective goal makes it possible to eventually find a suitable algorithm that is en par or even outperforms the human.
Additionally, manual decisions are not generalizable to other tasks and not scalable to larger datasets which in turn is the reason to use machine learning to begin with.

However, \todo{}

- accuracy vs. semantic accuracy
- debugging models / data
- domain expert
- understanding /trusting models