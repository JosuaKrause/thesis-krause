\begin{quote}\textit{
Recently, there is growing consensus of the critical need to have better techniques to explain machine learning models.  However, many of the popular techniques are instance-level explanations, which explain the model from the point of view of a single data point.  While local explanations may be misleading, they are also not human-scale, as it is impossible for users to read explanations for how the model behaves on all of their data points.  Our paper explores the effectiveness of providing instance-level explanations in aggregate, by demonstrating that such aggregated explanations have a significant impact on users' ability to detect biases in data.
This is achieved by comparing meaningful subsets, such as differences between ground truth labels, predicted labels, and correct and incorrect predictions, which provide necessary navigation to explain machine learning models.
}
\end{quote}

\begin{contributions}{What is the impact of aggregating instance-level explanations?}
\item Histograms can effectively be used to aggregate instance-level explanations and compare subsets.
\item Aggregating instance-level explanations significantly outperform inspecting individual explanations or aggregating without explanations in detecting biases in the input data.
\item Inspecting individual instance-level explanations can be misleading and hurt detecting biases in the input data.
\item Aggregated instance-level explanations allow to detect biases equally well as in a table without explanations while being more scalable.
\end{contributions}

\begin{quote}
\textit{Josua Krause, Adam Perer, Enrico Bertini}
\end{quote}

% Recently, instance-level explanations have become a popular tool to inspect machine learning models and build a mental model of their behavior.
% However, those explanations are typically presented to the user one instance at a time.
% In order to make informed decisions about an analyzed machine learning model a data scientist needs to review examples in order to gain a holistic view of the model.
% This might be possible with relatively small data sets but becomes in-feasible when a user has to look at ${\sim}1,000$ instances.
% We explore how aggregating and providing navigation for those instance-level explanations can be an effective way of trusting model decisions and demonstrate its ability to help detect data sets with inherent biases using a controlled study on Amazon Mechanical Turk.
% For this, we generated data sets with known biases and let participants compare them to their unbiased counterparts.
% Enrico's title:
% "A User Study on the Effect of Visual Aggregation and Feature Weighting on Interpretation of Machine Learning Models"
% Adam's attempt:
% Explaining Predictions in Aggregate: A User Study on the Effect of Explanations for Interpreting Machine Learning Models
